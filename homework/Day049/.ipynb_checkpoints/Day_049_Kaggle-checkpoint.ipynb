{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit (AMD64)]\n",
      "pandas version: 0.24.2\n",
      "matplotlib version: 3.0.3\n",
      "NumPy version: 1.16.2\n",
      "SciPy version: 1.2.1\n",
      "IPython version: 7.4.0\n",
      "scikit-learn version: 0.20.3\n",
      "-------------------------\n",
      "['house_test.csv.gz', 'house_train.csv.gz', 'taxi_data1.csv', 'taxi_data2.csv', 'titanic_test.csv', 'titanic_train.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "\n",
    "#load packages\n",
    "import sys #access to system parameters https://docs.python.org/3/library/sys.html\n",
    "print(\"Python version: {}\". format(sys.version))\n",
    "\n",
    "import pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\n",
    "print(\"pandas version: {}\". format(pd.__version__))\n",
    "\n",
    "import matplotlib #collection of functions for scientific and publication-ready visualization\n",
    "print(\"matplotlib version: {}\". format(matplotlib.__version__))\n",
    "\n",
    "import numpy as np #foundational package for scientific computing\n",
    "print(\"NumPy version: {}\". format(np.__version__))\n",
    "\n",
    "import scipy as sp #collection of functions for scientific computing and advance mathematics\n",
    "print(\"SciPy version: {}\". format(sp.__version__)) \n",
    "\n",
    "import IPython\n",
    "from IPython import display #pretty printing of dataframes in Jupyter notebook\n",
    "print(\"IPython version: {}\". format(IPython.__version__)) \n",
    "\n",
    "import sklearn #collection of machine learning algorithms\n",
    "print(\"scikit-learn version: {}\". format(sklearn.__version__))\n",
    "\n",
    "#misc libraries\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print('-'*25)\n",
    "\n",
    "\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "\n",
    "import os\n",
    "print(os.listdir('CSV_DATA/Part02'))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-2dbb8a1849d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Common Model Algorithms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msvm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneighbors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnaive_bayes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensemble\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminant_analysis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgaussian_process\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#Common Model Helpers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "#Common Model Algorithms\n",
    "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#Common Model Helpers\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "#Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "#Configure Visualization Defaults\n",
    "#%matplotlib inline = show plots in Jupyter Notebook browser\n",
    "%matplotlib inline\n",
    "mpl.style.use('ggplot')\n",
    "sns.set_style('white')\n",
    "pylab.rcParams['figure.figsize'] = 12,8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'../input/train.csv' does not exist: b'../input/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-11b8a073d29b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata_raw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../input/train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#a dataset should be broken into 3 splits: train, test, and (final) validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'../input/train.csv' does not exist: b'../input/train.csv'"
     ]
    }
   ],
   "source": [
    "#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n",
    "data_raw = pd.read_csv('../input/train.csv')\n",
    "\n",
    "\n",
    "#a dataset should be broken into 3 splits: train, test, and (final) validation\n",
    "#the test file provided is the validation file for competition submission\n",
    "#we will split the train set into train and test data in future sections\n",
    "data_val  = pd.read_csv('../input/test.csv')\n",
    "\n",
    "\n",
    "#to play with our data we'll create a copy\n",
    "#remember python assignment or equal passes by reference vs values, so we use the copy function: https://stackoverflow.com/questions/46327494/python-pandas-dataframe-copydeep-false-vs-copydeep-true-vs\n",
    "data1 = data_raw.copy(deep = True)\n",
    "\n",
    "#however passing by reference is convenient, because we can clean both datasets at once\n",
    "data_cleaner = [data1, data_val]\n",
    "\n",
    "\n",
    "#preview data\n",
    "print (data_raw.info()) #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html\n",
    "#data_raw.head() #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.head.html\n",
    "#data_raw.tail() #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.tail.html\n",
    "data_raw.sample(10) #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train columns with null values:\\n', data1.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "\n",
    "print('Test/Validation columns with null values:\\n', data_val.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "\n",
    "data_raw.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###COMPLETING: complete or delete missing values in train and test/validation dataset\n",
    "for dataset in data_cleaner:    \n",
    "    #complete missing age with median\n",
    "    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n",
    "\n",
    "    #complete embarked with mode\n",
    "    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n",
    "\n",
    "    #complete missing fare with median\n",
    "    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n",
    "    \n",
    "#delete the cabin feature/column and others previously stated to exclude in train dataset\n",
    "drop_column = ['PassengerId','Cabin', 'Ticket']\n",
    "data1.drop(drop_column, axis=1, inplace = True)\n",
    "\n",
    "print(data1.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "print(data_val.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###CREATE: Feature Engineering for train and test/validation dataset\n",
    "for dataset in data_cleaner:    \n",
    "    #Discrete variables\n",
    "    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n",
    "\n",
    "    dataset['IsAlone'] = 1 #initialize to yes/1 is alone\n",
    "    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no/0 if family size is greater than 1\n",
    "\n",
    "    #quick and dirty code split title from name: http://www.pythonforbeginners.com/dictionary/python-split\n",
    "    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n",
    "\n",
    "\n",
    "    #Continuous variable bins; qcut vs cut: https://stackoverflow.com/questions/30211923/what-is-the-difference-between-pandas-qcut-and-pandas-cut\n",
    "    #Fare Bins/Buckets using qcut or frequency bins: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.qcut.html\n",
    "    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n",
    "\n",
    "    #Age Bins/Buckets using cut or value bins: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html\n",
    "    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)\n",
    "\n",
    "\n",
    "    \n",
    "#cleanup rare title names\n",
    "#print(data1['Title'].value_counts())\n",
    "stat_min = 10 #while small is arbitrary, we'll use the common minimum in statistics: http://nicholasjjackson.com/2012/03/08/sample-size-is-10-a-magic-number/\n",
    "title_names = (data1['Title'].value_counts() < stat_min) #this will create a true false series with title name as index\n",
    "\n",
    "#apply and lambda functions are quick and dirty code to find and replace with fewer lines of code: https://community.modeanalytics.com/python/tutorial/pandas-groupby-and-python-lambda-functions/\n",
    "data1['Title'] = data1['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\n",
    "print(data1['Title'].value_counts())\n",
    "print(\"-\"*10)\n",
    "\n",
    "\n",
    "#preview data again\n",
    "data1.info()\n",
    "data_val.info()\n",
    "data1.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERT: convert objects to category using Label Encoder for train and test/validation dataset\n",
    "\n",
    "#code categorical data\n",
    "label = LabelEncoder()\n",
    "for dataset in data_cleaner:    \n",
    "    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n",
    "    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n",
    "    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n",
    "    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n",
    "    dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])\n",
    "\n",
    "\n",
    "#define y variable aka target/outcome\n",
    "Target = ['Survived']\n",
    "\n",
    "#define x variables for original features aka feature selection\n",
    "data1_x = ['Sex','Pclass', 'Embarked', 'Title','SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone'] #pretty name/values for charts\n",
    "data1_x_calc = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code','SibSp', 'Parch', 'Age', 'Fare'] #coded for algorithm calculation\n",
    "data1_xy =  Target + data1_x\n",
    "print('Original X Y: ', data1_xy, '\\n')\n",
    "\n",
    "\n",
    "#define x variables for original w/bin features to remove continuous variables\n",
    "data1_x_bin = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code', 'FamilySize', 'AgeBin_Code', 'FareBin_Code']\n",
    "data1_xy_bin = Target + data1_x_bin\n",
    "print('Bin X Y: ', data1_xy_bin, '\\n')\n",
    "\n",
    "\n",
    "#define x and y variables for dummy features original\n",
    "data1_dummy = pd.get_dummies(data1[data1_x])\n",
    "data1_x_dummy = data1_dummy.columns.tolist()\n",
    "data1_xy_dummy = Target + data1_x_dummy\n",
    "print('Dummy X Y: ', data1_xy_dummy, '\\n')\n",
    "\n",
    "\n",
    "\n",
    "data1_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train columns with null values: \\n', data1.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "print (data1.info())\n",
    "print(\"-\"*10)\n",
    "\n",
    "print('Test/Validation columns with null values: \\n', data_val.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "print (data_val.info())\n",
    "print(\"-\"*10)\n",
    "\n",
    "data_raw.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train and test data with function defaults\n",
    "#random_state -> seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\n",
    "train1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\n",
    "train1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\n",
    "train1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy = model_selection.train_test_split(data1_dummy[data1_x_dummy], data1[Target], random_state = 0)\n",
    "\n",
    "\n",
    "print(\"Data1 Shape: {}\".format(data1.shape))\n",
    "print(\"Train1 Shape: {}\".format(train1_x.shape))\n",
    "print(\"Test1 Shape: {}\".format(test1_x.shape))\n",
    "\n",
    "train1_x_bin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discrete Variable Correlation by Survival using\n",
    "#group by aka pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\n",
    "for x in data1_x:\n",
    "    if data1[x].dtype != 'float64' :\n",
    "        print('Survival Correlation by:', x)\n",
    "        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\n",
    "        print('-'*10, '\\n')\n",
    "        \n",
    "\n",
    "#using crosstabs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html\n",
    "print(pd.crosstab(data1['Title'],data1[Target[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTANT: Intentionally plotted different ways for learning purposes only. \n",
    "\n",
    "#optional plotting w/pandas: https://pandas.pydata.org/pandas-docs/stable/visualization.html\n",
    "\n",
    "#we will use matplotlib.pyplot: https://matplotlib.org/api/pyplot_api.html\n",
    "\n",
    "#to organize our graphics will use figure: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure\n",
    "#subplot: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html#matplotlib.pyplot.subplot\n",
    "#and subplotS: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplots.html?highlight=matplotlib%20pyplot%20subplots#matplotlib.pyplot.subplots\n",
    "\n",
    "#graph distribution of quantitative data\n",
    "plt.figure(figsize=[16,12])\n",
    "\n",
    "plt.subplot(231)\n",
    "plt.boxplot(x=data1['Fare'], showmeans = True, meanline = True)\n",
    "plt.title('Fare Boxplot')\n",
    "plt.ylabel('Fare ($)')\n",
    "\n",
    "plt.subplot(232)\n",
    "plt.boxplot(data1['Age'], showmeans = True, meanline = True)\n",
    "plt.title('Age Boxplot')\n",
    "plt.ylabel('Age (Years)')\n",
    "\n",
    "plt.subplot(233)\n",
    "plt.boxplot(data1['FamilySize'], showmeans = True, meanline = True)\n",
    "plt.title('Family Size Boxplot')\n",
    "plt.ylabel('Family Size (#)')\n",
    "\n",
    "plt.subplot(234)\n",
    "plt.hist(x = [data1[data1['Survived']==1]['Fare'], data1[data1['Survived']==0]['Fare']], \n",
    "         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\n",
    "plt.title('Fare Histogram by Survival')\n",
    "plt.xlabel('Fare ($)')\n",
    "plt.ylabel('# of Passengers')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(235)\n",
    "plt.hist(x = [data1[data1['Survived']==1]['Age'], data1[data1['Survived']==0]['Age']], \n",
    "         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\n",
    "plt.title('Age Histogram by Survival')\n",
    "plt.xlabel('Age (Years)')\n",
    "plt.ylabel('# of Passengers')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(236)\n",
    "plt.hist(x = [data1[data1['Survived']==1]['FamilySize'], data1[data1['Survived']==0]['FamilySize']], \n",
    "         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\n",
    "plt.title('Family Size Histogram by Survival')\n",
    "plt.xlabel('Family Size (#)')\n",
    "plt.ylabel('# of Passengers')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will use seaborn graphics for multi-variable comparison: https://seaborn.pydata.org/api.html\n",
    "\n",
    "#graph individual features by survival\n",
    "fig, saxis = plt.subplots(2, 3,figsize=(16,12))\n",
    "\n",
    "sns.barplot(x = 'Embarked', y = 'Survived', data=data1, ax = saxis[0,0])\n",
    "sns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=data1, ax = saxis[0,1])\n",
    "sns.barplot(x = 'IsAlone', y = 'Survived', order=[1,0], data=data1, ax = saxis[0,2])\n",
    "\n",
    "sns.pointplot(x = 'FareBin', y = 'Survived',  data=data1, ax = saxis[1,0])\n",
    "sns.pointplot(x = 'AgeBin', y = 'Survived',  data=data1, ax = saxis[1,1])\n",
    "sns.pointplot(x = 'FamilySize', y = 'Survived', data=data1, ax = saxis[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph distribution of qualitative data: Pclass\n",
    "#we know class mattered in survival, now let's compare class and a 2nd feature\n",
    "fig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))\n",
    "\n",
    "sns.boxplot(x = 'Pclass', y = 'Fare', hue = 'Survived', data = data1, ax = axis1)\n",
    "axis1.set_title('Pclass vs Fare Survival Comparison')\n",
    "\n",
    "sns.violinplot(x = 'Pclass', y = 'Age', hue = 'Survived', data = data1, split = True, ax = axis2)\n",
    "axis2.set_title('Pclass vs Age Survival Comparison')\n",
    "\n",
    "sns.boxplot(x = 'Pclass', y ='FamilySize', hue = 'Survived', data = data1, ax = axis3)\n",
    "axis3.set_title('Pclass vs Family Size Survival Comparison')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph distribution of qualitative data: Sex\n",
    "#we know sex mattered in survival, now let's compare sex and a 2nd feature\n",
    "fig, qaxis = plt.subplots(1,3,figsize=(14,12))\n",
    "\n",
    "sns.barplot(x = 'Sex', y = 'Survived', hue = 'Embarked', data=data1, ax = qaxis[0])\n",
    "axis1.set_title('Sex vs Embarked Survival Comparison')\n",
    "\n",
    "sns.barplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data=data1, ax  = qaxis[1])\n",
    "axis1.set_title('Sex vs Pclass Survival Comparison')\n",
    "\n",
    "sns.barplot(x = 'Sex', y = 'Survived', hue = 'IsAlone', data=data1, ax  = qaxis[2])\n",
    "axis1.set_title('Sex vs IsAlone Survival Comparison')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more side-by-side comparisons\n",
    "fig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))\n",
    "\n",
    "#how does family size factor with sex & survival compare\n",
    "sns.pointplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=data1,\n",
    "              palette={\"male\": \"blue\", \"female\": \"pink\"},\n",
    "              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis1)\n",
    "\n",
    "#how does class factor with sex & survival compare\n",
    "sns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data1,\n",
    "              palette={\"male\": \"blue\", \"female\": \"pink\"},\n",
    "              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how does embark port factor with class, sex, and survival compare\n",
    "#facetgrid: https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\n",
    "e = sns.FacetGrid(data1, col = 'Embarked')\n",
    "e.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', ci=95.0, palette = 'deep')\n",
    "e.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot distributions of age of passengers who survived or did not survive\n",
    "a = sns.FacetGrid( data1, hue = 'Survived', aspect=4 )\n",
    "a.map(sns.kdeplot, 'Age', shade= True )\n",
    "a.set(xlim=(0 , data1['Age'].max()))\n",
    "a.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram comparison of sex, class, and age by survival\n",
    "h = sns.FacetGrid(data1, row = 'Sex', col = 'Pclass', hue = 'Survived')\n",
    "h.map(plt.hist, 'Age', alpha = .75)\n",
    "h.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pair plots of entire dataset\n",
    "pp = sns.pairplot(data1, hue = 'Survived', palette = 'deep', size=1.2, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\n",
    "pp.set(xticklabels=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation heatmap of dataset\n",
    "def correlation_heatmap(df):\n",
    "    _ , ax = plt.subplots(figsize =(14, 12))\n",
    "    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n",
    "    \n",
    "    _ = sns.heatmap(\n",
    "        df.corr(), \n",
    "        cmap = colormap,\n",
    "        square=True, \n",
    "        cbar_kws={'shrink':.9 }, \n",
    "        ax=ax,\n",
    "        annot=True, \n",
    "        linewidths=0.1,vmax=1.0, linecolor='white',\n",
    "        annot_kws={'fontsize':12 }\n",
    "    )\n",
    "    \n",
    "    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "\n",
    "correlation_heatmap(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Machine Learning Algorithm (MLA) Selection and Initialization\n",
    "MLA = [\n",
    "    #Ensemble Methods\n",
    "    ensemble.AdaBoostClassifier(),\n",
    "    ensemble.BaggingClassifier(),\n",
    "    ensemble.ExtraTreesClassifier(),\n",
    "    ensemble.GradientBoostingClassifier(),\n",
    "    ensemble.RandomForestClassifier(),\n",
    "\n",
    "    #Gaussian Processes\n",
    "    gaussian_process.GaussianProcessClassifier(),\n",
    "    \n",
    "    #GLM\n",
    "    linear_model.LogisticRegressionCV(),\n",
    "    linear_model.PassiveAggressiveClassifier(),\n",
    "    linear_model.RidgeClassifierCV(),\n",
    "    linear_model.SGDClassifier(),\n",
    "    linear_model.Perceptron(),\n",
    "    \n",
    "    #Navies Bayes\n",
    "    naive_bayes.BernoulliNB(),\n",
    "    naive_bayes.GaussianNB(),\n",
    "    \n",
    "    #Nearest Neighbor\n",
    "    neighbors.KNeighborsClassifier(),\n",
    "    \n",
    "    #SVM\n",
    "    svm.SVC(probability=True),\n",
    "    svm.NuSVC(probability=True),\n",
    "    svm.LinearSVC(),\n",
    "    \n",
    "    #Trees    \n",
    "    tree.DecisionTreeClassifier(),\n",
    "    tree.ExtraTreeClassifier(),\n",
    "    \n",
    "    #Discriminant Analysis\n",
    "    discriminant_analysis.LinearDiscriminantAnalysis(),\n",
    "    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n",
    "\n",
    "    \n",
    "    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n",
    "    XGBClassifier()    \n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "#split dataset in cross-validation with this splitter class: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n",
    "#note: this is an alternative to train_test_split\n",
    "cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%\n",
    "\n",
    "#create table to compare MLA metrics\n",
    "MLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\n",
    "MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
    "\n",
    "#create table to compare MLA predictions\n",
    "MLA_predict = data1[Target]\n",
    "\n",
    "#index through MLA and save performance to table\n",
    "row_index = 0\n",
    "for alg in MLA:\n",
    "\n",
    "    #set name and parameters\n",
    "    MLA_name = alg.__class__.__name__\n",
    "    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n",
    "    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n",
    "    \n",
    "    #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n",
    "    cv_results = model_selection.cross_validate(alg, data1[data1_x_bin], data1[Target], cv  = cv_split)\n",
    "\n",
    "    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n",
    "    #if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n",
    "    \n",
    "\n",
    "    #save MLA predictions - see section 6 for usage\n",
    "    alg.fit(data1[data1_x_bin], data1[Target])\n",
    "    MLA_predict[MLA_name] = alg.predict(data1[data1_x_bin])\n",
    "    \n",
    "    row_index+=1\n",
    "\n",
    "    \n",
    "#print and sort table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html\n",
    "MLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\n",
    "MLA_compare\n",
    "#MLA_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\n",
    "sns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n",
    "\n",
    "#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\n",
    "plt.title('Machine Learning Algorithm Accuracy Score \\n')\n",
    "plt.xlabel('Accuracy Score (%)')\n",
    "plt.ylabel('Algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTANT: This is a handmade model for learning purposes only.\n",
    "#However, it is possible to create your own predictive model without a fancy algorithm :)\n",
    "\n",
    "#coin flip model with random 1/survived 0/died\n",
    "\n",
    "#iterate over dataFrame rows as (index, Series) pairs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html\n",
    "for index, row in data1.iterrows(): \n",
    "    #random number generator: https://docs.python.org/2/library/random.html\n",
    "    if random.random() > .5:     # Random float x, 0.0 <= x < 1.0    \n",
    "        data1.set_value(index, 'Random_Predict', 1) #predict survived/1\n",
    "    else: \n",
    "        data1.set_value(index, 'Random_Predict', 0) #predict died/0\n",
    "    \n",
    "\n",
    "#score random guess of survival. Use shortcut 1 = Right Guess and 0 = Wrong Guess\n",
    "#the mean of the column will then equal the accuracy\n",
    "data1['Random_Score'] = 0 #assume prediction wrong\n",
    "data1.loc[(data1['Survived'] == data1['Random_Predict']), 'Random_Score'] = 1 #set to 1 for correct prediction\n",
    "print('Coin Flip Model Accuracy: {:.2f}%'.format(data1['Random_Score'].mean()*100))\n",
    "\n",
    "#we can also use scikit's accuracy_score function to save us a few lines of code\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score\n",
    "print('Coin Flip Model Accuracy w/SciKit: {:.2f}%'.format(metrics.accuracy_score(data1['Survived'], data1['Random_Predict'])*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by or pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\n",
    "pivot_female = data1[data1.Sex=='female'].groupby(['Sex','Pclass', 'Embarked','FareBin'])['Survived'].mean()\n",
    "print('Survival Decision Tree w/Female Node: \\n',pivot_female)\n",
    "\n",
    "pivot_male = data1[data1.Sex=='male'].groupby(['Sex','Title'])['Survived'].mean()\n",
    "print('\\n\\nSurvival Decision Tree w/Male Node: \\n',pivot_male)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handmade data model using brain power (and Microsoft Excel Pivot Tables for quick calculations)\n",
    "def mytree(df):\n",
    "    \n",
    "    #initialize table to store predictions\n",
    "    Model = pd.DataFrame(data = {'Predict':[]})\n",
    "    male_title = ['Master'] #survived titles\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        #Question 1: Were you on the Titanic; majority died\n",
    "        Model.loc[index, 'Predict'] = 0\n",
    "\n",
    "        #Question 2: Are you female; majority survived\n",
    "        if (df.loc[index, 'Sex'] == 'female'):\n",
    "                  Model.loc[index, 'Predict'] = 1\n",
    "\n",
    "        #Question 3A Female - Class and Question 4 Embarked gain minimum information\n",
    "\n",
    "        #Question 5B Female - FareBin; set anything less than .5 in female node decision tree back to 0       \n",
    "        if ((df.loc[index, 'Sex'] == 'female') & \n",
    "            (df.loc[index, 'Pclass'] == 3) & \n",
    "            (df.loc[index, 'Embarked'] == 'S')  &\n",
    "            (df.loc[index, 'Fare'] > 8)\n",
    "\n",
    "           ):\n",
    "                  Model.loc[index, 'Predict'] = 0\n",
    "\n",
    "        #Question 3B Male: Title; set anything greater than .5 to 1 for majority survived\n",
    "        if ((df.loc[index, 'Sex'] == 'male') &\n",
    "            (df.loc[index, 'Title'] in male_title)\n",
    "            ):\n",
    "            Model.loc[index, 'Predict'] = 1\n",
    "        \n",
    "        \n",
    "    return Model\n",
    "\n",
    "\n",
    "#model data\n",
    "Tree_Predict = mytree(data1)\n",
    "print('Decision Tree Model Accuracy/Precision Score: {:.2f}%\\n'.format(metrics.accuracy_score(data1['Survived'], Tree_Predict)*100))\n",
    "\n",
    "\n",
    "#Accuracy Summary Report with http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report\n",
    "#Where recall score = (true positives)/(true positive + false negative) w/1 being best:http://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score\n",
    "#And F1 score = weighted average of precision and recall w/1 being best: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
    "print(metrics.classification_report(data1['Survived'], Tree_Predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Accuracy Summary\n",
    "#Credit: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = metrics.confusion_matrix(data1['Survived'], Tree_Predict)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "class_names = ['Dead', 'Survived']\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True, \n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base model\n",
    "dtree = tree.DecisionTreeClassifier(random_state = 0)\n",
    "base_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split)\n",
    "dtree.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "print('BEFORE DT Parameters: ', dtree.get_params())\n",
    "print(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "#print(\"BEFORE DT Test w/bin set score min: {:.2f}\". format(base_results['test_score'].min()*100))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#tune hyper-parameters: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
    "param_grid = {'criterion': ['gini', 'entropy'],  #scoring methodology; two supported formulas for calculating information gain - default is gini\n",
    "              #'splitter': ['best', 'random'], #splitting methodology; two supported strategies - default is best\n",
    "              'max_depth': [2,4,6,8,10,None], #max depth tree can grow; default is none\n",
    "              #'min_samples_split': [2,5,10,.03,.05], #minimum subset size BEFORE new split (fraction is % of total); default is 2\n",
    "              #'min_samples_leaf': [1,5,10,.03,.05], #minimum subset size AFTER new split split (fraction is % of total); default is 1\n",
    "              #'max_features': [None, 'auto'], #max features to consider when performing split; default none or all\n",
    "              'random_state': [0] #seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\n",
    "             }\n",
    "\n",
    "#print(list(model_selection.ParameterGrid(param_grid)))\n",
    "\n",
    "#choose best model with grid_search: #http://scikit-learn.org/stable/modules/grid_search.html#grid-search\n",
    "#http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\n",
    "tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)\n",
    "tune_model.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "#print(tune_model.cv_results_.keys())\n",
    "#print(tune_model.cv_results_['params'])\n",
    "print('AFTER DT Parameters: ', tune_model.best_params_)\n",
    "#print(tune_model.cv_results_['mean_train_score'])\n",
    "print(\"AFTER DT Training w/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n",
    "#print(tune_model.cv_results_['mean_test_score'])\n",
    "print(\"AFTER DT Test w/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n",
    "print(\"AFTER DT Test w/bin score 3*std: +/- {:.2f}\". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#duplicates gridsearchcv\n",
    "#tune_results = model_selection.cross_validate(tune_model, data1[data1_x_bin], data1[Target], cv  = cv_split)\n",
    "\n",
    "#print('AFTER DT Parameters: ', tune_model.best_params_)\n",
    "#print(\"AFTER DT Training w/bin set score mean: {:.2f}\". format(tune_results['train_score'].mean()*100)) \n",
    "#print(\"AFTER DT Test w/bin set score mean: {:.2f}\". format(tune_results['test_score'].mean()*100))\n",
    "#print(\"AFTER DT Test w/bin set score min: {:.2f}\". format(tune_results['test_score'].min()*100))\n",
    "#print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base model\n",
    "print('BEFORE DT RFE Training Shape Old: ', data1[data1_x_bin].shape) \n",
    "print('BEFORE DT RFE Training Columns Old: ', data1[data1_x_bin].columns.values)\n",
    "\n",
    "print(\"BEFORE DT RFE Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"BEFORE DT RFE Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "\n",
    "#feature selection\n",
    "dtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = 'accuracy', cv = cv_split)\n",
    "dtree_rfe.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "#transform x&y to reduced features and fit new model\n",
    "#alternative: can use pipeline to reduce fit and transform steps: http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "X_rfe = data1[data1_x_bin].columns.values[dtree_rfe.get_support()]\n",
    "rfe_results = model_selection.cross_validate(dtree, data1[X_rfe], data1[Target], cv  = cv_split)\n",
    "\n",
    "#print(dtree_rfe.grid_scores_)\n",
    "print('AFTER DT RFE Training Shape New: ', data1[X_rfe].shape) \n",
    "print('AFTER DT RFE Training Columns New: ', X_rfe)\n",
    "\n",
    "print(\"AFTER DT RFE Training w/bin score mean: {:.2f}\". format(rfe_results['train_score'].mean()*100)) \n",
    "print(\"AFTER DT RFE Test w/bin score mean: {:.2f}\". format(rfe_results['test_score'].mean()*100))\n",
    "print(\"AFTER DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(rfe_results['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#tune rfe model\n",
    "rfe_tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)\n",
    "rfe_tune_model.fit(data1[X_rfe], data1[Target])\n",
    "\n",
    "#print(rfe_tune_model.cv_results_.keys())\n",
    "#print(rfe_tune_model.cv_results_['params'])\n",
    "print('AFTER DT RFE Tuned Parameters: ', rfe_tune_model.best_params_)\n",
    "#print(rfe_tune_model.cv_results_['mean_train_score'])\n",
    "print(\"AFTER DT RFE Tuned Training w/bin score mean: {:.2f}\". format(rfe_tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n",
    "#print(rfe_tune_model.cv_results_['mean_test_score'])\n",
    "print(\"AFTER DT RFE Tuned Test w/bin score mean: {:.2f}\". format(rfe_tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n",
    "print(\"AFTER DT RFE Tuned Test w/bin score 3*std: +/- {:.2f}\". format(rfe_tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\n",
    "import graphviz \n",
    "dot_data = tree.export_graphviz(dtree, out_file=None, \n",
    "                                feature_names = data1_x_bin, class_names = True,\n",
    "                                filled = True, rounded = True)\n",
    "graph = graphviz.Source(dot_data) \n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#30\n",
    "#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\n",
    "#there are some 1's, but enough blues and light reds to create a \"super algorithm\" by combining them\n",
    "correlation_heatmap(MLA_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#31\n",
    "#why choose one model, when you can pick them all with voting classifier\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n",
    "#removed models w/o attribute 'predict_proba' required for vote classifier and models with a 1.0 correlation to another model\n",
    "vote_est = [\n",
    "    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\n",
    "    ('ada', ensemble.AdaBoostClassifier()),\n",
    "    ('bc', ensemble.BaggingClassifier()),\n",
    "    ('etc',ensemble.ExtraTreesClassifier()),\n",
    "    ('gbc', ensemble.GradientBoostingClassifier()),\n",
    "    ('rfc', ensemble.RandomForestClassifier()),\n",
    "\n",
    "    #Gaussian Processes: http://scikit-learn.org/stable/modules/gaussian_process.html#gaussian-process-classification-gpc\n",
    "    ('gpc', gaussian_process.GaussianProcessClassifier()),\n",
    "    \n",
    "    #GLM: http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "    ('lr', linear_model.LogisticRegressionCV()),\n",
    "    \n",
    "    #Navies Bayes: http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "    ('bnb', naive_bayes.BernoulliNB()),\n",
    "    ('gnb', naive_bayes.GaussianNB()),\n",
    "    \n",
    "    #Nearest Neighbor: http://scikit-learn.org/stable/modules/neighbors.html\n",
    "    ('knn', neighbors.KNeighborsClassifier()),\n",
    "    \n",
    "    #SVM: http://scikit-learn.org/stable/modules/svm.html\n",
    "    ('svc', svm.SVC(probability=True)),\n",
    "    \n",
    "    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n",
    "   ('xgb', XGBClassifier())\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "#Hard Vote or majority rules\n",
    "vote_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\n",
    "vote_hard_cv = model_selection.cross_validate(vote_hard, data1[data1_x_bin], data1[Target], cv  = cv_split)\n",
    "vote_hard.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "print(\"Hard Voting Training w/bin score mean: {:.2f}\". format(vote_hard_cv['train_score'].mean()*100)) \n",
    "print(\"Hard Voting Test w/bin score mean: {:.2f}\". format(vote_hard_cv['test_score'].mean()*100))\n",
    "print(\"Hard Voting Test w/bin score 3*std: +/- {:.2f}\". format(vote_hard_cv['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#Soft Vote or weighted probabilities\n",
    "vote_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\n",
    "vote_soft_cv = model_selection.cross_validate(vote_soft, data1[data1_x_bin], data1[Target], cv  = cv_split)\n",
    "vote_soft.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "print(\"Soft Voting Training w/bin score mean: {:.2f}\". format(vote_soft_cv['train_score'].mean()*100)) \n",
    "print(\"Soft Voting Test w/bin score mean: {:.2f}\". format(vote_soft_cv['test_score'].mean()*100))\n",
    "print(\"Soft Voting Test w/bin score 3*std: +/- {:.2f}\". format(vote_soft_cv['test_score'].std()*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WARNING: Running is very computational intensive and time expensive.\n",
    "#Code is written for experimental/developmental purposes and not production ready!\n",
    "\n",
    "\n",
    "#Hyperparameter Tune with GridSearchCV: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "grid_n_estimator = [10, 50, 100, 300]\n",
    "grid_ratio = [.1, .25, .5, .75, 1.0]\n",
    "grid_learn = [.01, .03, .05, .1, .25]\n",
    "grid_max_depth = [2, 4, 6, 8, 10, None]\n",
    "grid_min_samples = [5, 10, .03, .05, .10]\n",
    "grid_criterion = ['gini', 'entropy']\n",
    "grid_bool = [True, False]\n",
    "grid_seed = [0]\n",
    "\n",
    "\n",
    "grid_param = [\n",
    "            [{\n",
    "            #AdaBoostClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "            'n_estimators': grid_n_estimator, #default=50\n",
    "            'learning_rate': grid_learn, #default=1\n",
    "            #'algorithm': ['SAMME', 'SAMME.R'], #default=SAMME.R\n",
    "            'random_state': grid_seed\n",
    "            }],\n",
    "       \n",
    "    \n",
    "            [{\n",
    "            #BaggingClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier\n",
    "            'n_estimators': grid_n_estimator, #default=10\n",
    "            'max_samples': grid_ratio, #default=1.0\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "\n",
    "    \n",
    "            [{\n",
    "            #ExtraTreesClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier\n",
    "            'n_estimators': grid_n_estimator, #default=10\n",
    "            'criterion': grid_criterion, #default=gini\n",
    "            'max_depth': grid_max_depth, #default=None\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "\n",
    "\n",
    "            [{\n",
    "            #GradientBoostingClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n",
    "            #'loss': ['deviance', 'exponential'], #default=deviance\n",
    "            'learning_rate': [.05], #default=0.1 -- 12/31/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n",
    "            'n_estimators': [300], #default=100 -- 12/31/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n",
    "            #'criterion': ['friedman_mse', 'mse', 'mae'], #default=friedman_mse\n",
    "            'max_depth': grid_max_depth, #default=3   \n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "\n",
    "    \n",
    "            [{\n",
    "            #RandomForestClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
    "            'n_estimators': grid_n_estimator, #default=10\n",
    "            'criterion': grid_criterion, #default=gini\n",
    "            'max_depth': grid_max_depth, #default=None\n",
    "            'oob_score': [True], #default=False -- 12/31/17 set to reduce runtime -- The best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'oob_score': True, 'random_state': 0} with a runtime of 146.35 seconds.\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "    \n",
    "            [{    \n",
    "            #GaussianProcessClassifier\n",
    "            'max_iter_predict': grid_n_estimator, #default: 100\n",
    "            'random_state': grid_seed\n",
    "            }],\n",
    "        \n",
    "    \n",
    "            [{\n",
    "            #LogisticRegressionCV - http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV\n",
    "            'fit_intercept': grid_bool, #default: True\n",
    "            #'penalty': ['l1','l2'],\n",
    "            'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], #default: lbfgs\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "            \n",
    "    \n",
    "            [{\n",
    "            #BernoulliNB - http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB\n",
    "            'alpha': grid_ratio, #default: 1.0\n",
    "             }],\n",
    "    \n",
    "    \n",
    "            #GaussianNB - \n",
    "            [{}],\n",
    "    \n",
    "            [{\n",
    "            #KNeighborsClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n",
    "            'n_neighbors': [1,2,3,4,5,6,7], #default: 5\n",
    "            'weights': ['uniform', 'distance'], #default = uniform\n",
    "            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "            }],\n",
    "            \n",
    "    \n",
    "            [{\n",
    "            #SVC - http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n",
    "            #http://blog.hackerearth.com/simple-tutorial-svm-parameter-tuning-python-r\n",
    "            #'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "            'C': [1,2,3,4,5], #default=1.0\n",
    "            'gamma': grid_ratio, #edfault: auto\n",
    "            'decision_function_shape': ['ovo', 'ovr'], #default:ovr\n",
    "            'probability': [True],\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "\n",
    "    \n",
    "            [{\n",
    "            #XGBClassifier - http://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "            'learning_rate': grid_learn, #default: .3\n",
    "            'max_depth': [1,2,4,6,8,10], #default 2\n",
    "            'n_estimators': grid_n_estimator, \n",
    "            'seed': grid_seed  \n",
    "             }]   \n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "start_total = time.perf_counter() #https://docs.python.org/3/library/time.html#time.perf_counter\n",
    "for clf, param in zip (vote_est, grid_param): #https://docs.python.org/3/library/functions.html#zip\n",
    "\n",
    "    #print(clf[1]) #vote_est is a list of tuples, index 0 is the name and index 1 is the algorithm\n",
    "    #print(param)\n",
    "    \n",
    "    \n",
    "    start = time.perf_counter()        \n",
    "    best_search = model_selection.GridSearchCV(estimator = clf[1], param_grid = param, cv = cv_split, scoring = 'roc_auc')\n",
    "    best_search.fit(data1[data1_x_bin], data1[Target])\n",
    "    run = time.perf_counter() - start\n",
    "\n",
    "    best_param = best_search.best_params_\n",
    "    print('The best parameter for {} is {} with a runtime of {:.2f} seconds.'.format(clf[1].__class__.__name__, best_param, run))\n",
    "    clf[1].set_params(**best_param) \n",
    "\n",
    "\n",
    "run_total = time.perf_counter() - start_total\n",
    "print('Total optimization time was {:.2f} minutes.'.format(run_total/60))\n",
    "\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hard Vote or majority rules w/Tuned Hyperparameters\n",
    "grid_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\n",
    "grid_hard_cv = model_selection.cross_validate(grid_hard, data1[data1_x_bin], data1[Target], cv  = cv_split)\n",
    "grid_hard.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "print(\"Hard Voting w/Tuned Hyperparameters Training w/bin score mean: {:.2f}\". format(grid_hard_cv['train_score'].mean()*100)) \n",
    "print(\"Hard Voting w/Tuned Hyperparameters Test w/bin score mean: {:.2f}\". format(grid_hard_cv['test_score'].mean()*100))\n",
    "print(\"Hard Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- {:.2f}\". format(grid_hard_cv['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "#Soft Vote or weighted probabilities w/Tuned Hyperparameters\n",
    "grid_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\n",
    "grid_soft_cv = model_selection.cross_validate(grid_soft, data1[data1_x_bin], data1[Target], cv  = cv_split)\n",
    "grid_soft.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "print(\"Soft Voting w/Tuned Hyperparameters Training w/bin score mean: {:.2f}\". format(grid_soft_cv['train_score'].mean()*100)) \n",
    "print(\"Soft Voting w/Tuned Hyperparameters Test w/bin score mean: {:.2f}\". format(grid_soft_cv['test_score'].mean()*100))\n",
    "print(\"Soft Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- {:.2f}\". format(grid_soft_cv['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#12/31/17 tuned with data1_x_bin\n",
    "#The best parameter for AdaBoostClassifier is {'learning_rate': 0.1, 'n_estimators': 300, 'random_state': 0} with a runtime of 33.39 seconds.\n",
    "#The best parameter for BaggingClassifier is {'max_samples': 0.25, 'n_estimators': 300, 'random_state': 0} with a runtime of 30.28 seconds.\n",
    "#The best parameter for ExtraTreesClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0} with a runtime of 64.76 seconds.\n",
    "#The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 34.35 seconds.\n",
    "#The best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'oob_score': True, 'random_state': 0} with a runtime of 76.32 seconds.\n",
    "#The best parameter for GaussianProcessClassifier is {'max_iter_predict': 10, 'random_state': 0} with a runtime of 6.01 seconds.\n",
    "#The best parameter for LogisticRegressionCV is {'fit_intercept': True, 'random_state': 0, 'solver': 'liblinear'} with a runtime of 8.04 seconds.\n",
    "#The best parameter for BernoulliNB is {'alpha': 0.1} with a runtime of 0.19 seconds.\n",
    "#The best parameter for GaussianNB is {} with a runtime of 0.04 seconds.\n",
    "#The best parameter for KNeighborsClassifier is {'algorithm': 'brute', 'n_neighbors': 7, 'weights': 'uniform'} with a runtime of 4.84 seconds.\n",
    "#The best parameter for SVC is {'C': 2, 'decision_function_shape': 'ovo', 'gamma': 0.1, 'probability': True, 'random_state': 0} with a runtime of 29.39 seconds.\n",
    "#The best parameter for XGBClassifier is {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300, 'seed': 0} with a runtime of 46.23 seconds.\n",
    "#Total optimization time was 5.56 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for modeling\n",
    "print(data_val.info())\n",
    "print(\"-\"*10)\n",
    "#data_val.sample(10)\n",
    "\n",
    "\n",
    "\n",
    "#handmade decision tree - submission score = 0.77990\n",
    "data_val['Survived'] = mytree(data_val).astype(int)\n",
    "\n",
    "\n",
    "#decision tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\n",
    "#submit_dt = tree.DecisionTreeClassifier()\n",
    "#submit_dt = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_dt.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_dt.best_params_) #Best Parameters:  {'criterion': 'gini', 'max_depth': 4, 'random_state': 0}\n",
    "#data_val['Survived'] = submit_dt.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "#bagging w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77990\n",
    "#submit_bc = ensemble.BaggingClassifier()\n",
    "#submit_bc = model_selection.GridSearchCV(ensemble.BaggingClassifier(), param_grid= {'n_estimators':grid_n_estimator, 'max_samples': grid_ratio, 'oob_score': grid_bool, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_bc.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_bc.best_params_) #Best Parameters:  {'max_samples': 0.25, 'n_estimators': 500, 'oob_score': True, 'random_state': 0}\n",
    "#data_val['Survived'] = submit_bc.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "#extra tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\n",
    "#submit_etc = ensemble.ExtraTreesClassifier()\n",
    "#submit_etc = model_selection.GridSearchCV(ensemble.ExtraTreesClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_etc.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_etc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\n",
    "#data_val['Survived'] = submit_etc.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "#random foreset w/full dataset modeling submission score: defaults= 0.71291, tuned= 0.73205\n",
    "#submit_rfc = ensemble.RandomForestClassifier()\n",
    "#submit_rfc = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_rfc.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_rfc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\n",
    "#data_val['Survived'] = submit_rfc.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "\n",
    "#ada boosting w/full dataset modeling submission score: defaults= 0.74162, tuned= 0.75119\n",
    "#submit_abc = ensemble.AdaBoostClassifier()\n",
    "#submit_abc = model_selection.GridSearchCV(ensemble.AdaBoostClassifier(), param_grid={'n_estimators': grid_n_estimator, 'learning_rate': grid_ratio, 'algorithm': ['SAMME', 'SAMME.R'], 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_abc.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_abc.best_params_) #Best Parameters:  {'algorithm': 'SAMME.R', 'learning_rate': 0.1, 'n_estimators': 300, 'random_state': 0}\n",
    "#data_val['Survived'] = submit_abc.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "#gradient boosting w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77033\n",
    "#submit_gbc = ensemble.GradientBoostingClassifier()\n",
    "#submit_gbc = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid={'learning_rate': grid_ratio, 'n_estimators': grid_n_estimator, 'max_depth': grid_max_depth, 'random_state':grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_gbc.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_gbc.best_params_) #Best Parameters:  {'learning_rate': 0.25, 'max_depth': 2, 'n_estimators': 50, 'random_state': 0}\n",
    "#data_val['Survived'] = submit_gbc.predict(data_val[data1_x_bin])\n",
    "\n",
    "#extreme boosting w/full dataset modeling submission score: defaults= 0.73684, tuned= 0.77990\n",
    "#submit_xgb = XGBClassifier()\n",
    "#submit_xgb = model_selection.GridSearchCV(XGBClassifier(), param_grid= {'learning_rate': grid_learn, 'max_depth': [0,2,4,6,8,10], 'n_estimators': grid_n_estimator, 'seed': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_xgb.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_xgb.best_params_) #Best Parameters:  {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300, 'seed': 0}\n",
    "#data_val['Survived'] = submit_xgb.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "#hard voting classifier w/full dataset modeling submission score: defaults= 0.75598, tuned = 0.77990\n",
    "#data_val['Survived'] = vote_hard.predict(data_val[data1_x_bin])\n",
    "data_val['Survived'] = grid_hard.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "#soft voting classifier w/full dataset modeling submission score: defaults= 0.73684, tuned = 0.74162\n",
    "#data_val['Survived'] = vote_soft.predict(data_val[data1_x_bin])\n",
    "#data_val['Survived'] = grid_soft.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "#submit file\n",
    "submit = data_val[['PassengerId','Survived']]\n",
    "submit.to_csv(\"../working/submit.csv\", index=False)\n",
    "\n",
    "print('Validation Data Distribution: \\n', data_val['Survived'].value_counts(normalize = True))\n",
    "submit.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
